{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEUKbWrG+yLCptO9XLGiO3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franfram/Transformers-nlp/blob/main/DeepNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j8x8ffoH-XI",
        "outputId": "8e80c370-449f-45f5-97c7-f8d7edf8a9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.5.2-py3-none-any.whl (432 kB)\n",
            "\u001b[K     |████████████████████████████████| 432 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.2.2-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 37.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (5.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 70.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 41.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Installing collected packages: urllib3, xxhash, tokenizers, responses, multiprocess, huggingface-hub, transformers, sentencepiece, datasets, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.5.2 evaluate-0.2.2 huggingface-hub-0.10.0 multiprocess-0.70.13 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.22.2 urllib3-1.25.11 xxhash-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "checkpoint = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DwM6-R4MIJOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"hola como estas\" #@param {type: \"string\"}\n",
        "\n",
        "# tokenization process is done by the tokenize() method of the tokenizer\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)\n",
        "# decoding is the other way around of encoding, from vocabulary indices, we want to get a string. This can be done with the decode() method\n",
        "decoded_string = tokenizer.decode(ids)\n",
        "\n",
        "print(decoded_string)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvCbhD7iNyoc",
        "outputId": "a98363f5-1ccb-4f59-b401-6fbfa33004b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1734, 1151, 1932]\n",
            "hola como estas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we’ve seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called tokens. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\n",
        "\n",
        "The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method. Again, we need to use the same vocabulary used when the model was pretrained.\n",
        "\n",
        "To get a better understanding of the two steps, we’ll explore them separately. Note that we will use some methods that perform parts of the tokenization pipeline separately to show you the intermediate results of those steps, but in practice, you should call the tokenizer directly on your inputs (as shown in the section 2).\n"
      ],
      "metadata": {
        "id": "oalqOTmtp6PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare inputs\n",
        "raw_inputs = [\n",
        "    \"quiero comer \",\n",
        "    \"yo quiero\",\n",
        "]\n",
        "\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpPVoPHtQ98P",
        "outputId": "5b892ca1-290f-4e25-e44f-f4bfd13de058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[   4, 1563, 1987,    5],\n",
            "        [   4, 1252, 1563,    5]]), 'token_type_ids': tensor([[0, 0, 0, 0],\n",
            "        [0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
            "        [1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.last_hidden_state.shape) #[batch size, sequence length, hidden size]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e28sTb80R3Lc",
        "outputId": "fcff8dfc-e0eb-4f85-853b-9a580b4a1138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3107, -0.0482, -0.2685,  ..., -1.2886,  0.7412, -0.8510],\n",
            "         [-0.4834, -0.1054, -0.0760,  ..., -0.3920, -0.1903, -0.4548],\n",
            "         [-0.4013, -0.4127, -0.4304,  ..., -0.2887, -0.7466, -0.2979],\n",
            "         [-0.3224, -0.0176, -0.0630,  ..., -0.6006, -0.0127, -0.1626]],\n",
            "\n",
            "        [[ 0.8068,  0.8387,  0.1999,  ...,  0.5771,  1.1830, -0.1755],\n",
            "         [ 0.5724, -0.2247, -0.0648,  ...,  0.0827,  0.2711, -0.5267],\n",
            "         [ 0.4977,  0.0194,  0.0029,  ...,  0.3079,  1.0262, -0.6613],\n",
            "         [ 0.1892,  0.7965, -0.0492,  ...,  0.7769,  1.2281, -0.4855]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.2322,  0.1761, -0.3326,  ..., -0.2519,  0.1239,  0.1035],\n",
            "        [-0.5266,  0.1396, -0.5428,  ..., -0.4514,  0.7699,  0.2066]],\n",
            "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
            "torch.Size([2, 4, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vars(outputs)\n",
        "\n",
        "print(outputs.last_hidden_state) # creo que son los logits; the raw, unnormalized scores outputted by the last layer of the model\n",
        "\n",
        "# to convert logits to probabilities, they need to go through a softmax layer\n",
        "import torch\n",
        "predictions = torch.nn.functional.softmax(outputs.last_hidden_state, dim=-1)\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8h9IqTtS2fd",
        "outputId": "567c5069-c0ac-43fb-889c-ca54b2886667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.3107, -0.0482, -0.2685,  ..., -1.2886,  0.7412, -0.8510],\n",
            "         [-0.4834, -0.1054, -0.0760,  ..., -0.3920, -0.1903, -0.4548],\n",
            "         [-0.4013, -0.4127, -0.4304,  ..., -0.2887, -0.7466, -0.2979],\n",
            "         [-0.3224, -0.0176, -0.0630,  ..., -0.6006, -0.0127, -0.1626]],\n",
            "\n",
            "        [[ 0.8068,  0.8387,  0.1999,  ...,  0.5771,  1.1830, -0.1755],\n",
            "         [ 0.5724, -0.2247, -0.0648,  ...,  0.0827,  0.2711, -0.5267],\n",
            "         [ 0.4977,  0.0194,  0.0029,  ...,  0.3079,  1.0262, -0.6613],\n",
            "         [ 0.1892,  0.7965, -0.0492,  ...,  0.7769,  1.2281, -0.4855]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "tensor([[[0.0014, 0.0010, 0.0008,  ..., 0.0003, 0.0021, 0.0004],\n",
            "         [0.0006, 0.0009, 0.0010,  ..., 0.0007, 0.0009, 0.0007],\n",
            "         [0.0007, 0.0007, 0.0007,  ..., 0.0008, 0.0005, 0.0008],\n",
            "         [0.0008, 0.0010, 0.0010,  ..., 0.0006, 0.0010, 0.0009]],\n",
            "\n",
            "        [[0.0023, 0.0023, 0.0012,  ..., 0.0018, 0.0033, 0.0009],\n",
            "         [0.0018, 0.0008, 0.0010,  ..., 0.0011, 0.0013, 0.0006],\n",
            "         [0.0017, 0.0011, 0.0010,  ..., 0.0014, 0.0029, 0.0005],\n",
            "         [0.0013, 0.0023, 0.0010,  ..., 0.0023, 0.0035, 0.0006]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that we seem to need"
      ],
      "metadata": {
        "id": "bziThTvMa9fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Define model checkpoint\n",
        "checkpoint = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Define model\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "seq1 = \"yo quiero\" #@param {type: \"string\"} \n",
        "seq2 = \"quiero ir a\" #@param {type: \"string\"}\n",
        "\n",
        "sequences = [\n",
        "    seq1, \n",
        "    seq2\n",
        "]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "output = model(**tokens)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGAOsQena9Jj",
        "outputId": "87f21029-613e-4b5e-cb64-0713ca7bc303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.8068,  0.8387,  0.1999,  ...,  0.5771,  1.1830, -0.1755],\n",
            "         [ 0.5724, -0.2247, -0.0648,  ...,  0.0827,  0.2711, -0.5267],\n",
            "         [ 0.4977,  0.0194,  0.0029,  ...,  0.3079,  1.0262, -0.6613],\n",
            "         [ 0.1892,  0.7965, -0.0492,  ...,  0.7769,  1.2281, -0.4855],\n",
            "         [ 0.2981,  0.7136, -0.0072,  ...,  0.7018,  1.0105, -0.2157]],\n",
            "\n",
            "        [[ 0.4667,  0.0963, -0.1903,  ..., -1.1062,  0.5403,  0.3907],\n",
            "         [ 0.3396,  0.0048, -0.3239,  ..., -0.1553,  0.3557,  0.0766],\n",
            "         [ 0.1299, -0.3269, -0.4146,  ..., -0.4310, -0.3135,  0.6949],\n",
            "         [-0.2690, -0.4313,  0.3067,  ...,  0.1194,  0.4537, -0.0747],\n",
            "         [ 0.1312,  0.0034, -0.3424,  ..., -0.6802,  0.6460, -0.0816]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.2017, -0.1935,  0.3371,  ...,  0.0971,  0.6859,  0.2973],\n",
            "        [ 0.4669,  0.4709,  0.0425,  ...,  0.1149,  0.4237,  0.5939]],\n",
            "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "checkpoint = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zSSCx59HbLw9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}